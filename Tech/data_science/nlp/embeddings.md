Word embedding is the collective name for a set of language modeling and feature learning techniques in natural language processing (NLP) where words or phrases from the vocabulary are mapped to vectors of real numbers.

Embeddings are very widely used in NLP, as each word is represented as a vector. 
Two famous embedding examples are Glove and word2vec. 
